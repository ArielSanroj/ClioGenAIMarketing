from fastapi import FastAPI, HTTPException
from bs4 import BeautifulSoup
import requests
import re
from typing import List, Optional
from pydantic import BaseModel
import json

class Location(BaseModel):
    name: str
    address: str
    phone: Optional[str] = None

class SocialMedia(BaseModel):
    platform: str
    url: str
    icon: str

class Product(BaseModel):
    name: str
    description: str
    price: float
    image: str
    rating: float
    reviewCount: int

class CompanyData(BaseModel):
    background: str
    mission: str
    values: List[str]
    locations: List[Location]
    socialMedia: List[SocialMedia]
    products: List[Product]

app = FastAPI()

async def extract_company_info(url: str) -> CompanyData:
    """Extract detailed company information from website"""
    try:
        # Make request to main URL
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract About Us information
        about_us = extract_about_us(soup, url)
        
        # Extract Locations
        locations = extract_locations(soup, url)
        
        # Extract Social Media
        social_media = extract_social_media(soup)
        
        # Extract Products
        products = extract_products(soup, url)

        return CompanyData(
            background=about_us['background'],
            mission=about_us['mission'],
            values=about_us['values'],
            locations=locations,
            socialMedia=social_media,
            products=products
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def extract_about_us(soup: BeautifulSoup, base_url: str) -> dict:
    """Extract About Us information"""
    # Try to find About Us page
    about_link = soup.find('a', text=re.compile(r'about', re.I))
    if about_link:
        about_url = about_link.get('href')
        if not about_url.startswith('http'):
            about_url = base_url + about_url
        
        about_response = requests.get(about_url)
        about_soup = BeautifulSoup(about_response.text, 'html.parser')
    else:
        about_soup = soup

    # Extract information
    background = extract_text_by_headers(about_soup, ['about us', 'our story', 'who we are'])
    mission = extract_text_by_headers(about_soup, ['our mission', 'mission statement'])
    values = extract_list_items(about_soup, ['our values', 'core values'])

    return {
        'background': background,
        'mission': mission,
        'values': values
    }

def extract_locations(soup: BeautifulSoup, base_url: str) -> List[Location]:
    """Extract company locations"""
    locations = []
    
    # Try to find Contact or Locations page
    contact_link = soup.find('a', text=re.compile(r'contact|locations', re.I))
    if contact_link:
        contact_url = contact_link.get('href')
        if not contact_url.startswith('http'):
            contact_url = base_url + contact_url
        
        contact_response = requests.get(contact_url)
        contact_soup = BeautifulSoup(contact_response.text, 'html.parser')
    else:
        contact_soup = soup

    # Look for address patterns
    address_patterns = [
        r'\d+\s+[A-Za-z]+\s+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd)',
        r'[A-Za-z]+,\s*[A-Z]{2}\s*\d{5}'
    ]

    for pattern in address_patterns:
        addresses = contact_soup.find_all(text=re.compile(pattern))
        for addr in addresses:
            locations.append(Location(
                name="Office",
                address=addr.strip(),
                phone=extract_phone_number(addr.parent)
            ))

    return locations

def extract_social_media(soup: BeautifulSoup) -> List[SocialMedia]:
    """Extract social media links"""
    social_platforms = {
        'facebook': r'facebook\.com',
        'twitter': r'twitter\.com',
        'linkedin': r'linkedin\.com',
        'instagram': r'instagram\.com'
    }

    social_media = []
    for platform, pattern in social_platforms.items():
        social_link = soup.find('a', href=re.compile(pattern))
        if social_link:
            social_media.append(SocialMedia(
                platform=platform.capitalize(),
                url=social_link['href'],
                icon=f'/icons/{platform}.svg'
            ))

    return social_media

def extract_products(soup: BeautifulSoup, base_url: str) -> List[Product]:
    """Extract product information"""
    products = []
    
    # Try to find Products page
    products_link = soup.find('a', text=re.compile(r'products|shop', re.I))
    if products_link:
        products_url = products_link.get('href')
        if not products_url.startswith('http'):
            products_url = base_url + products_url
        
        products_response = requests.get(products_url)
        products_soup = BeautifulSoup(products_response.text, 'html.parser')
    else:
        products_soup = soup

    # Look for product elements
    product_elements = products_soup.find_all(class_=re.compile(r'product|item'))
    
    for element in product_elements:
        name = element.find(text=re.compile(r'[A-Za-z]'))
        price_element = element.find(text=re.compile(r'\$\d+\.?\d*'))
        price = float(re.search(r'\$(\d+\.?\d*)', price_element).group(1)) if price_element else 0.0
        
        image = element.find('img')
        image_url = image['src'] if image else ''
        if image_url and not image_url.startswith('http'):
            image_url = base_url + image_url

        description = extract_text_by_class(element, ['description', 'details'])
        
        # Extract ratings if available
        rating = 0.0
        review_count = 0
        rating_element = element.find(class_=re.compile(r'rating|stars'))
        if rating_element:
            rating_text = rating_element.get_text()
            rating_match = re.search(r'(\d+\.?\d*)', rating_text)
            if rating_match:
                rating = float(rating_match.group(1))
            
            review_count_match = re.search(r'\((\d+)\)', rating_text)
            if review_count_match:
                review_count = int(review_count